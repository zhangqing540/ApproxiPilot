#GNN-model

The GNN-Model Component serves as the core machine learning engine of the ApproxiPilot framework, implementing circuit-path-aware Graph Neural Network models for predicting hardware performance metrics. This component bridges the gap between raw circuit data generated by the Dataset Generation component and the multi-objective optimization performed by the Design Space Exploration component.The component provides predictive models for four key hardware metrics: area, power consumption, latency, and SSIM (Structural Similarity Index Measure) quality metrics.

This codebase is built on the PyTorch Geometric framework and implements a variety of Graph Neural Network (GNN) models, which are applicable to scenarios such as latency prediction, area estimation, and power consumption analysis. For different datasets including sobel, gaussian, and kmeans, the codebase provides corresponding model variants.

Graph Convolutional Network (GCN) achieves hierarchical propagation and aggregation of node features through multiple GCNConv layers. The network uses the ReLU activation function to handle non-linear features and ultimately outputs prediction results via a fully connected layer, supporting batch processing and neighbor node feature aggregation.

Graph Attention Network (GAT/GATv2) is constructed based on GATConv or GATv2Conv layers, dynamically assigning weights to neighbor nodes through an attention mechanism. The model supports multi-head attention to capture multi-dimensional neighbor relationships, adopts the LeakyReLU activation function to enhance non-linear expression capabilities, and strengthens feature learning through a multi-layer convolutional structure.

GraphSAGE is designed for large-scale graph data and implements sampling and aggregation strategies based on SAGEConv layers. The model combines multi-layer convolutional operations with fully connected layers for feature transformation, supports batch processing and custom neighbor sampling counts, and ensures feature learning performance while maintaining computational efficiency.

Message Passing Neural Network (MPNN) inherits from the MessagePassing base class and builds a general framework using the "add" aggregation strategy. The model implements feature transformation via a Multi-Layer Perceptron (MLP) and customizes a message-passing function that combines neighbor node features with normalization coefficients, providing a flexible message-processing mechanism for different graph tasks.
